{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import requests\n",
    "import re\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "# silence all of pdfminerâ€™s page-parsing warnings\n",
    "logging.getLogger(\"pdfminer.pdfpage\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", message=\"CropBox missing from /Page, defaulting to MediaBox\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05329eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df with all links\n",
    "year = 2024\n",
    "mainurl = f'https://www.cmdachennai.gov.in/ApprovedLayout/{year}.htm'\n",
    "df = pd.read_html(mainurl)[0]\n",
    "df = df.iloc[:-1,:-1]\n",
    "df\n",
    "\n",
    "\n",
    "resp = requests.get(mainurl)\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "pdflist = soup.find_all('a')\n",
    "pdflist = [i for i in pdflist if i.get('href').endswith('.pdf')]\n",
    "df['bs4'] = pdflist\n",
    "\n",
    "def extractpplfromurl(url):\n",
    "    return url.split('/')[-1][:-4].replace('-','/')\n",
    "\n",
    "df['url'] = df.bs4.apply(lambda x: 'https://www.cmdachennai.gov.in/' + x.get('href'))\n",
    "df['extracted_ppl'] = df.url.apply(extractpplfromurl)\n",
    "df['pdf_savepath'] = df.extracted_ppl.apply(lambda x: x.split('/')[-1] +'/'+ x.split('/')[0] + '.pdf' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a105de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify layout numbers match\n",
    "assert pd.DataFrame(df.iloc[:,1] == df.iloc[:,-2]).all()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists('data/2024/01.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208508a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_file, x_start_ratio)  :        \n",
    "    all_text = []\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            width, height = page.width, page.height\n",
    "            try:\n",
    "                words = page.extract_words()\n",
    "                extentfound = [i for i in words if i['text'].lower().startswith('ext')][0]\n",
    "                bbox_start = int(extentfound['x0'] - 70)\n",
    "            except:\n",
    "                print(\"extent not found\")\n",
    "                bbox_start = width * x_start_ratio\n",
    "            finally:\n",
    "                crop = page.within_bbox((bbox_start, 0, width, height))\n",
    "                text = crop.extract_text()\n",
    "                if text:\n",
    "                    all_text.append(text)\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "\n",
    "async def download_and_extract_pdf(session, df_row, x_start_ratio=0.4, retries=3, backoff_factor=1.0):\n",
    "    \"\"\"\n",
    "    Downloads a PDF from a URL, saves it to disk, and extracts text from a cropped region of each page.\n",
    "    Retries the download on failure with exponential backoff.\n",
    "    \"\"\"\n",
    "    pdf_url = df_row['url']\n",
    "    savepath = os.path.join('data', df_row['pdf_savepath'])\n",
    "\n",
    "    saved_to_disk = False\n",
    "    # check if pdf already exists in disk\n",
    "    if os.path.exists(savepath):\n",
    "        saved_to_disk = True\n",
    "        print(f\"{savepath} already on disk, extracting text...\")\n",
    "        # extract text\n",
    "        textlist = extract_text_from_pdf(savepath, x_start_ratio)\n",
    "        return textlist, saved_to_disk\n",
    "\n",
    "    else:\n",
    "\n",
    "        for attempt in range(1, retries + 1):\n",
    "            try:\n",
    "                async with session.get(pdf_url, ssl=False) as response:\n",
    "                    response.raise_for_status()\n",
    "                    content = await response.read()\n",
    "\n",
    "                    # Save PDF to disk\n",
    "                    os.makedirs(os.path.dirname(os.path.abspath(savepath)), exist_ok=True)\n",
    "                    with open(savepath, 'wb') as fh:\n",
    "                        fh.write(content)\n",
    "                    saved_to_disk = True\n",
    "                    print(f\"Saved PDF to disk: {savepath}\")\n",
    "\n",
    "                    # Extract text from cropped region\n",
    "                    pdf_file_content = io.BytesIO(content)\n",
    "                    all_text = extract_text_from_pdf(pdf_file_content, x_start_ratio)\n",
    "\n",
    "                # Successfully downloaded and processed\n",
    "                return all_text, saved_to_disk\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt == retries:\n",
    "                    print(f\"Attempt {attempt} failed; no more retries.\")\n",
    "                    return f\"ERROR: {e}\", saved_to_disk\n",
    "                wait_time = backoff_factor * (2 ** (attempt - 1))\n",
    "                print(f\"Attempt {attempt} failed with error: {e}. Retrying in {wait_time} seconds...\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "\n",
    "\n",
    "def extract_multiple_patterns(text: str) -> dict[str|None]:\n",
    "    \"\"\"\n",
    "    Extract multiple patterns from PDF text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with pattern matches\n",
    "    \"\"\"\n",
    "    # Define patterns with descriptive keys\n",
    "    patterns = {\n",
    "        'total_no_plots': [\n",
    "            r'NO\\.?\\s*OF\\.?\\s*PLO(?:T|TS)[^\\n]*\\s*[=:]\\s*(\\d+)',\n",
    "            r'No\\.?\\s*of\\s*[Pp]lots\\s*[=:]\\s*(\\d+)',\n",
    "            r'TOTAL\\s*PLOTS?\\s*[=:]\\s*(\\d+)'\n",
    "        ],\n",
    "        'ews': [\n",
    "            r'(?i)\\bE\\.?W\\.?S\\.?\\s+(?:Provided|Plots)\\b[^\\n]*[:=]\\s*(\\d+)\\s*Nos\\.?\\b'\n",
    "        ],\n",
    "        'regular':[\n",
    "            r'(?i)REGULAR PLOTS.\\s*[^\\n]*?[:=]\\s*(\\d+)\\s*No'\n",
    "        ],\n",
    "        'date': [\n",
    "            r'DATE\\s*[=:]\\s*(\\d{1,2}\\s?[-\\/]\\s?\\d{2}\\s?[-\\/]\\s?\\d{4})',\n",
    "            r'Date\\s*:\\s*(\\d{1,2}(?:st|nd|rd|th)?\\s+(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{4})',\n",
    "            r'(\\d{4}-\\d{2}-\\d{2})'\n",
    "        ],\n",
    "        'reference_number': [\n",
    "            r'REF(?:ERENCE)?\\s*NO\\.?\\s*[=:]\\s*([A-Z0-9-/]+)',\n",
    "            r'File\\s*No\\.?\\s*[=:]\\s*([A-Z0-9-/]+)',\n",
    "            r'Application\\s*No\\.?\\s*[=:]\\s*([A-Z0-9-/]+)'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Dictionary to store results\n",
    "        results = {}\n",
    "        \n",
    "        # Search for each pattern category\n",
    "        for category, pattern_list in patterns.items():\n",
    "            results[category] = None\n",
    "            for pattern in pattern_list:\n",
    "                matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "                if matches:\n",
    "                    results[category] = matches[0]\n",
    "                    break\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {text}: {str(e)}\")\n",
    "        return {key: None for key in patterns.keys()}, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e31f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now populate the columns from the extracted patterns\n",
    "def extract_and_expand(text):\n",
    "    if text and not isinstance(text, str) or (isinstance(text, str) and not text.startswith(\"ERROR:\")):\n",
    "        patterns = extract_multiple_patterns(text)\n",
    "        return patterns\n",
    "    return {key: None for key in ['total_no_plots', 'ews', 'regular', 'date', 'reference_number']}\n",
    "\n",
    "# Usage with asyncio\n",
    "async def process_df(df):\n",
    "    results_text = []\n",
    "    results_saved = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [ download_and_extract_pdf(session, row) for _, row in df.iterrows()]\n",
    "        batch_results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Unpack results\n",
    "        batch_text, batch_saved = zip(*batch_results)\n",
    "        results_text.extend(batch_text)\n",
    "        results_saved.extend(batch_saved)\n",
    "        \n",
    "    result_df = df.copy()\n",
    "    result_df['extracted_text'] = results_text\n",
    "    result_df['saved_to_disk'] = results_saved\n",
    "    return result_df\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # Process first 20 rows as an example\n",
    "    # result_df = await process_df(df)\n",
    "    \n",
    "    # Now you can use the DataFrame as usual\n",
    "    print(f\"Successfully saved {result_df['saved_to_disk'].sum()} of {len(result_df)} PDFs\")\n",
    "    \n",
    "    # Apply pattern extraction to the extracted text and expand into separate columns\n",
    "    for key in ['total_no_plots', 'ews', 'regular', 'date', 'reference_number']:\n",
    "        result_df[key] = None  # Initialize columns\n",
    "\n",
    "\n",
    "    # Apply the extraction function\n",
    "    pattern_dicts = result_df['extracted_text'].apply(extract_and_expand)\n",
    "\n",
    "    # Expand the dictionaries into separate columns\n",
    "    for key in ['total_no_plots', 'ews', 'regular', 'date', 'reference_number']:\n",
    "        result_df[key] = pattern_dicts.apply(lambda x: x.get(key))\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Run the async function\n",
    "result_df = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d1e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqdf = result_df.drop(columns=['bs4', 'extracted_ppl'])\n",
    "# pqdf.to_parquet('./2024extract.parquet')\n",
    "# pqdf.to_excel('./pathtoxl.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a6659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pqdf = pd.read_parquet('./2024extract.parquet')\n",
    "pqdf[pqdf.extracted_text.str.startswith(\"ERROR\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cef57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqdf[pqdf.total_no_plots.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8bb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f3a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqdf.loc[204].apply(extract_and_expand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
